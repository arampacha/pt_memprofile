# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['MemHooks', 'plot_log', 'reset_grad', 'profile_memory', 'MemProfileCallback', 'MemStatsCallback']

# Cell
import torch
from torch import Tensor
import torch.nn as nn
from fastai.callback.all import Hooks, ShortEpochCallback, HookCallback
from fastai.basics import *

# Cell
def _generate_mem_hook(mem_log, idx, hook_type, experiment):
    "Hook function generator"
    def hook(module, *args):
        inp_shape = args[0][0].shape
        out_shape = args[1][0].shape if len(args)>1 else None
        if len(mem_log) == 0: call_idx = 0
        else: call_idx = mem_log[-1]["call_idx"] + 1
        mem_all = torch.cuda.memory_allocated()
        mem_reserved = torch.cuda.memory_reserved()
        torch.cuda.synchronize()
        mem_log.append({
            'layer_idx': idx,
            'call_idx': call_idx,
            'layer_type': type(module).__name__,
            'experiment': experiment,
            'hook_type': hook_type,
            'mem_all': mem_all,
            'mem_reservd': mem_reserved,
            'input_shape': inp_shape,
            'output_shape': out_shape,
        })
    return hook

# Cell
class MemHooks(Hooks):
    "Creates hooks for logging memory stats"
    def __init__(self, modules, name=None):
        self.hooks = []
        self.mem_log = []
        for i, m in enumerate(modules): self.register_memory_hooks(m, i, name=name)

    def register_memory_hooks(self, m, i, name=None):
        fs = {'pre':m.register_forward_pre_hook,
              'fwd':m.register_forward_hook,
              'bwd':m.register_backward_hook}
        for hook_type in ['pre', 'fwd', 'bwd']:
            self.hooks.append(fs[hook_type](_generate_mem_hook(self.mem_log, i, hook_type, name)))

# Cell
def plot_log(mem_log:pd.DataFrame):
    plt.plot(mem_log['call_idx'], mem_log['mem_all']/1024)
    plt.ylabel('Memory allocated (Kb)');

# Cell
def reset_grad(model:nn.Module):
    for p in model.parameters():
        p.grad = None

# Cell
def profile_memory(model:nn.Module, xb:Tensor, yb:Tensor, loss_func=CrossEntropyLossFlat(), plot=True, label=None):
    label = ifnone(label, type(model).__name__)
    device = xb.device
    model.to(device)
    prealloc = torch.cuda.memory_allocated()
    with MemHooks(flatten_model(model), label) as h:
        out = model(xb)
        loss = loss_func(out, yb)
        loss.backward()
        reset_grad(model)
        mem_log = pd.DataFrame(h.mem_log, copy=True)
    mem_log['mem_all'] = mem_log['mem_all'] - prealloc
    if plot:
        plot_log(mem_log)
    return mem_log

# Cell
class MemProfileCallback(Callback):
    "Cancels batch after backward to avoid opt.step()"
    def before_batch(self):
        # self._split(self.dls.one_batch())
        # self.model.to(self.dls.device)
        self.model.train()
        self.learn.training = True
    def after_backward(self):
        print('Batch canceled')
        raise CancelBatchException

# Cell
@patch
def profile_memory(self:Learner, plot=True):
    with MemHooks(flatten_model(self.model), type(self.model).__name__) as h:
        self._split(self.dls.one_batch())
        self.model.to(self.dls.device)
        prealloc = torch.cuda.memory_allocated()
        with self.added_cbs(MemProfileCallback()), self.no_logging():
            self('before_batch')
            try: self._do_one_batch()
            except CancelBatchException:
                for p in self.model.parameters(): p.grad=None
        mem_log = pd.DataFrame(h.mem_log, copy=True)
    mem_log['mem_all'] = mem_log['mem_all'] - prealloc
    if plot:
        plt.plot(mem_log['call_idx'], mem_log['mem_all']/1024)

    return mem_log

# Cell
class MemStatsCallback(HookCallback):
    "Registers memory hooks on modules"
    def __init__(self, modules=None, label=None, remove_end=True):
        store_attr()
        self.prealloc = torch.cuda.memory_allocated()
        self.every = None

    def _register(self): self.hooks = MemHooks(self.modules, name=self.label)

    def after_fit(self):
        self.stats = pd.DataFrame(self.hooks.mem_log, copy=True)
        self.stats['mem_all'] = self.stats['mem_all'] - self.prealloc
        if self.remove_end: self._remove()

    def plot(self): plot_log(self.stats)