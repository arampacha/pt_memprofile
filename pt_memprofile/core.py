# AUTOGENERATED! DO NOT EDIT! File to edit: 00_core.ipynb (unless otherwise specified).

__all__ = ['MemHooks', 'profile_memory', 'MemProfileCallback']

# Cell
import torch
from fastai.callback.all import Hooks
from fastai.basics import *

# Cell
def _generate_mem_hook(mem_log, idx, hook_type, experiment):
    "Hook function generator"
    def hook(module, *args):
        inp_shape = args[0][0].shape
        out_shape = args[1][0].shape if len(args)>1 else None
        if len(mem_log) == 0: call_idx = 0
        else: call_idx = mem_log[-1]["call_idx"] + 1
        mem_all = torch.cuda.memory_allocated()
        mem_reserved = torch.cuda.memory_reserved()
        torch.cuda.synchronize()
        mem_log.append({
            'layer_idx': idx,
            'call_idx': call_idx,
            'layer_type': type(module).__name__,
            'experiment': experiment,
            'hook_type': hook_type,
            'mem_all': mem_all,
            'mem_reservd': mem_reserved,
            'input_shape': inp_shape,
            'output_shape': out_shape,
        })
    return hook

# Cell
class MemHooks(Hooks):
    "Creates hooks for logging memory stats"
    def __init__(self, modules, name=None):
        self.hooks = []
        self.mem_log = []
        for i, m in enumerate(modules): self.register_memory_hooks(m, i, name=name)

    def register_memory_hooks(self, m, i, name=None):
        fs = {'pre':m.register_forward_pre_hook,
              'fwd':m.register_forward_hook,
              'bwd':m.register_backward_hook}
        for hook_type in ['pre', 'fwd', 'bwd']:
            self.hooks.append(fs[hook_type](_generate_mem_hook(self.mem_log, i, hook_type, name)))

# Cell
def profile_memory(learn, plot=True):
    with MemHooks(flatten_model(learn.model), type(learn.model).__name__) as h:
        xb, yb = learn.dls.one_batch()
        self.model.to(self.dls.device)
        out = learn.model(xb)
        loss = learn.loss_func(out, yb)
        loss.backward()
        mem_log = pd.DataFrame(h.mem_log, copy=True)
    if plot:
        plt.plot(mem_log['call_idx'], mem_log['mem_all'])
    return mem_log

# Cell
class MemProfileCallback(Callback):
    "Cancels batch after backward to avoid opt.step()"
    def before_batch(self):
        # learn._split(learn.dls.one_batch())
        # self.model.to(self.dls.device)
        self.model.train()
        self.learn.training = True
    def after_backward(self):
        print('Batch canceled')
        raise CancelBatchException

# Cell
@patch
def profile_memory(self:Learner, plot=True):
    with MemHooks(flatten_model(self.model), type(self.model).__name__) as h:
        self._split(self.dls.one_batch())
        self.model.to(self.dls.device)
        prealloc = torch.cuda.memory_allocated()
        with self.added_cbs(MemProfileCallback()), self.no_logging():
            self('before_batch')
            try: self._do_one_batch()
            except CancelBatchException:
                for p in self.model.parameters(): p.grad=None
        mem_log = pd.DataFrame(h.mem_log, copy=True)
        mem_log['mem_all'] = mem_log['mem_all'] - prealloc
    if plot:
        plt.plot(mem_log['call_idx'], mem_log['mem_all'])
    return mem_log